# -*- coding: utf-8 -*-
"""лаба2 (1) bd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I_neyEETZh7BgwP8YD7-UAivCSm1rXHR

# Парсинг веб-страниц с помощью Beautiful Soup и Pandas

`Веб-скрапинг` — это процесс использования методов для извлечения контента и данных с веб-сайта.

`Веб-скрапинг` — это автоматизированный способ извлечения больших объемов данных с веб-сайтов

В отличие от парсинга экрана, при которой копируются только пиксели, отображаемые на экране, парсинг веб-страниц извлекает базовый HTML-код и вместе с ним данные, хранящиеся в базе данных. Затем парсер может скопировать весь контент веб-сайта в другое место.

Парсинг веб-страниц используется во многих цифровых компаниях, которые полагаются на сбор данных. К законным случаям использования относятся:

- Боты поисковых систем сканируют сайт, анализируют его содержимое и затем ранжируют.
- Сайты сравнения цен, использующие ботов для автоматического получения цен и описаний продуктов для сайтов смежных продавцов.
- Компании, занимающиеся исследованием рынка, используют парсеры для получения данных с форумов и социальных сетей (например, для анализа настроений).

## Table of Contents
[1. Создание базы данных с помощью Beautiful Soup](#I.-Making-Database-From-Scratch-With-Beautiful-Soup) <br>
- [Получение данных](#Scrape-The-Data)
- [Создание баз данных](#Make-A-Database)

[2. Парсинг веб-страниц с использованием Pandas](#II.-Web-Scraping-Using-Pandas) <br>
- [Получение URL-адрес](#Get-The-URL)
- [Чтение HTML-страницы в Pandas](#Read-The-HTML-Webpage-Into-Pandas)
- [Очистка данных](#Data-Cleaning)
- [Быстрый анализ данных](#Quick-Exploratory-Data-Analysis)<br>

## I. Создание базы данных с помощью Beautiful Soup

Существует множество различных пакетов для парсинга веб-страниц, один из самых популярных — [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). Beautiful Soup анализирует веб-контент в объект Python и делает [DOM](https://en.wikipedia.org/wiki/Document_Object_Model) доступным для запроса поэлементно. При использовании в сочетании с пакетом запросов он делает парсинг веб-страниц очень простым!

---
### Установка Beautiful Soup
В `bash` терминале или `Anaconda Prompt`,запустить:
```bash
conda install beautifulsoup4
```
---
"""

# Импорт стандартных пакетов
import pandas as pd

# Для парсинга веб-страниц
import requests
import urllib.request
from bs4 import BeautifulSoup

# Для выполнения операций с регулярными выражениями
import re

# Визуализация данных
import seaborn as sns
import matplotlib.pyplot as plt

"""Извлечем **random функции  и их описание** из документации Python с веб-сайта [docs.python.org](https://docs.python.org/).

### Очистить данные
"""

# Сохранить URL-адрес веб-страницы, которую мы хотим извлечь в переменную.
url = 'https://docs.python.org/3/library/random.html#module-random'

"""При парсинге веб-страниц первым шагом является перенос содержимого страницы в переменную Python (строковую). Для более простых задач веб-скрапинга вы можете сделать это с помощью пакета Requests, который мы здесь будем использовать. Для более сложных задач (например, веб-страниц с большим количеством Javascript или других элементов, отображаемых веб-браузером) вам может потребоваться использовать что-то более сложное, например urllib или [Selenium](https://selenium-python .readthedocs.io/index.html)."""

# Отправление запроса на получение данных и присваивание ответ переменной.
response = requests.get(url)

response

response.content

"""Это нечитабельно. Именно здесь на помощь приходит `Beautiful Soup`.

`Beautiful Soup` помогает нам правильно анализировать содержимое страницы и приводить его в форму, которую нам будет легче использовать.
"""

# Преобразование содержимого в объект Beautiful Soup и присвойте его переменной.
soup = BeautifulSoup(response.content)
type(soup)

"""**Проверка результата.**"""

# Проверка переменной soup.

soup

# Другой способ загрузить html-код, используя 'urllib.request.urlopen()'

#url = urllib.request.urlopen("https://docs.python.org/3/library/random.html#module-random")
#soup = BeautifulSoup(url)
#soup

"""**Все еще сложно, но уже легче воспринимать.**

Однако настоящее преимущество `Beautiful Soup` заключается в том, что он *анализирует* нашу веб-страницу в соответствии с ее структурой и позволяет нам *искать* и *извлекать* элементы внутри нее. Это связано с тем, что он преобразует веб-страницу из строки в специальный объект `Beautiful Soup`.

Чтобы извлечь элементы HTML с веб-страницы, необходимо вызвать метод `.find()` для нашего объекта `Beautiful Soup`. Этот метод находит первый элемент, соответствующий переданному нами критерию. Критерием может быть элемент `id`,  `class`, тег `name`, или даже функция. (Полный список элементов поиска см. на [этой странице](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-the-tree).)

Но как нам узнать, какой элемент искать? Здесь  пригодится функция вашего браузера `Inspect` или `Inspect Element`. Просто щелкните правой кнопкой мыши интересующий объект на веб-странице и выберите `Inspect` в Chrome или `Inspect Element` в Firefox. Это покажет соответствующее место в `HTML`-коде, где появляется элемент. Оттуда можно найти идентификатор или имя класса, которые позволят  найти элемент с помощью `Beautiful Soup`.

**В данном случае настроить таргетинг на тег/элемент `dt`, как показано на рисунке ниже:**

<br>
<br>
<img src = "https://docs.google.com/uc?export=download&id=1Mj9K4QnhS5mFMK4Ddx_6OHFUIN-xpR9X" />

<br>
<br>

**Похоже, что ищем элемент `dt` с `id='random.___'`. Можно легко получить это с помощью команды Beautiful Soup `.findAll`.**
"""

# Найдите все имена функций — указываем имя элемента в данном случае «dt»

names = soup.body.findAll('dt')

print(names)

"""**Еще есть над чем поработать! Именно здесь вступает в силу регулярное выражение.**

"""

# Найдите всю искомую информацию с помощью регулярного выражения
# В этом случае каждая строка начинается с id='random.'

function_names = re.findall('id="random.\w+' , str(names)) # '\w+', означает, что строка должна заканчиваться именем функции.

print(function_names)

"""**Почти решили задачу! Необходимо удалить первые несколько символов из каждой строки.**"""

# Использование методов списка для редактирования наших значений:

function_names = [item[4:] for item in function_names]

print(function_names)

"""**Отлично! Теперь нам нужно сделать то же самое с описанием функции.
Нам нужно указать детали описания с помощью тега `dd`**.

<br>


<img src = "https://docs.google.com/uc?export=download&id=169-W93jfnmbwHejyP4QV4sDQm9LgriwB" />

<br>
<br>
"""

# Найти все описания функций

description = soup.body.findAll('dd')

#print(description)

"""**Сложно! Здесь много тегов (теги `<em>`). Чтобы избавиться от этих ненужных элементов из описанного выше метода вручную, потребуется много времени.**
    
К счастью, BeautifulSoup не только красив, но и умен. Давайте посмотрим на метод `.text`:
"""

# Создание списка

function_usage = []

# Создание цикла

for item in description:
    item = item.text      #  Сохраняем извлеченный текст в переменную
    item = item.replace('\n', ' ')     # чтобы избавиться от оператора следующей строки, которым является `\n`
    function_usage.append(item)

print(function_usage)  # Не перегружайтесь! это всего лишь описания функций из приведенных выше названий функций.

# Давайте проверим длину function_names и function_usage.

print(f' Length of function_names: {len(function_names)}')
print(f' Length of function_usage: {len(function_usage)}')

"""### Создание базы данных"""

# Создайте фрейм данных, поскольку длина обеих переменных одинакова!

data = pd.DataFrame( {  'function name': function_names,
                      'function usage' : function_usage  } )

data

# Давайте создадим файл CSV из фрейма данных.

data.to_csv('random_function.csv')

"""**БОНУС: если вы хотите настроить таргетинг на определенные атрибуты, например `is="bookkeeping-functions"`, вы можете использовать следующий код:**"""

# Особые атрибуты таргетинга

#example = soup.body.findAll ('div', attrs = {'id' : 'bookeeping-functions'})
#print(example)    # можно получить более точный результат поиска с помощью BeautifulSoup

"""## II. Парсинг веб-страниц с использованием Pandas

`Pandas` - это очень полезно! Можно легко очистить данные, используя функцию pandas `read_html()`.

Собираем данные о статистике игроков `NBA` и выполним быстрый анализ данных с веб-сайта [basketball-reference.com](https://www.basketball-reference.com).

### Получить URL-адрес

Во-первых, мы хотим проверить конкретный [URL](https://www.basketball-reference.com/leagues/NBA_2020_per_game.html), данные по которому мы собираемся собрать — статистику игроков `NBA` за сезон 2019–2020 гг.
"""

# Способ 1: только 1 год

# URL статистики игрока в 2020 году

url = 'https://www.basketball-reference.com/leagues/NBA_2020_per_game.html'
url

# Способ 2: несколько лет

years = ['2016', '2017', '2018', '2019', '2020']
str = 'https://www.basketball-reference.com/leagues/NBA_{}_per_game.html'

for year in years:
    url = str.format(year)
    print(url)

"""Чтение веб-страницы HTML в Pandas"""

# проверим URL-адрес статистики игрока в 2020 году

url = 'https://www.basketball-reference.com/leagues/NBA_2020_per_game.html'

df = pd.read_html(url, header = 0)

print(df)

"""**Выглядит неаккуратно. На самом деле есть список DataFrames. Можно улучшить восприятие этого объекта, используя Pandas (без дополнительных библиотек!)**"""

# Проверка количества DataFrame в этом списке.

print(f'number of tables in df: {len(df)}')

print('================')

# Поскольку здесь только 1, вытянем 0-й элемент:
df[0].head(20)

"""Есть  пропущенные значения (NaN) и несколько вхождений имен некоторых игроков, поскольку они были частью разных команд в одном и том же году.

### Очистка данных

*На сайте заголовок повторяется у каждых 20 игроков.Придется удалить последующие заголовки и оставить только первый заголовок:**

<br>
<br>
<img src = "https://docs.google.com/uc?export=download&id=1CEQs7TNFr4Nak0sQK10QYXl06uXcvrLN" />

<br>
<br>
"""

# Присваиваем таблицу переменной df_2020

df_2020 = df[0]

# Проверим заголовок таблицы, который представлен несколько раз в нескольких строках.

df_2020[df_2020.Age == 'Age'].head() # Весь последующий заголовок таблицы выбран для всего этого кадра данных!

# Проверяем длину имеющегося заголовка:

print(f' общее количество избыточных заголовков: {len(df_2020[df_2020.Age == "Age"])} ')

# Удалите избыточные заголовки в кадре данных:
df_2020_new = df_2020.drop(df_2020[df_2020.Age == 'Age'].index)

# Сравните до и после удаления избыточных заголовков с количеством строк:

print(f' всего строк df_2020: {df_2020.shape[0]} ')
print(f' общее количество строк df_2020_new: {df_2020_new.shape[0]} ')
print('=========================================')

df_2020_new.head(20)

"""### Быстрый исследовательский анализ данных"""

# Создание простой гистограммы

plt.figure(figsize=(10,8))

sns.distplot(df_2020_new.PTS,    # Проверка частоты полученнных очков игроком
            kde= False,          # Должно быть False, потому что  хотим сохранить исходную частоту. ( "kde=True" => это будет вероятность)
            hist_kws = dict( edgecolor = 'black', linewidth=2))

plt.title('ГИСТОГРАММА ОЧКОВ ЗА ИГРУ В СЕЗОНЕ NBA 2020 ГОДА')
plt.ylabel('КОЛИЧЕСТВО ИГРОКОВ')
plt.xlabel('ОЧКИ ЗА ИГРУ')
plt.show()

"""Из гистограммы мы видим:
- Около 57 игроков имеют от 0 до 1 очка.
- Есть менее 10 игроков, набравших 30 очков.
"""